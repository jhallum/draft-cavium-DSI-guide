{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Cavium Data Science cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Cavium Data Science infrastructure allows for the distributed processing of large data sets across a cluster of computers using simple programming models. The infrastructure is designed to scale up from single servers to over 30 machines, each offering local computation and storage. Rather than rely on hardware to deliver high-availability, the library itself is designed to detect and handle failures at the application layer, delivering a highly-available service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following software is currently installed on the cluster:\n",
    "* Hadoop 2.8.3  http://hadoop.apache.org/docs/r2.8.3/index.html\n",
    "* Hive 2.x.x  http://hive.apache.org/   (note: We recommend Hive on Tez for it's improved performance, rather than hive on hadoop.  We do not support hive on spark.)\n",
    "* Spark 2.x.x http://spark.apache.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to get an account"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Using the Cavium Hadoop cluster requires an ARC-TS user account (acquired by filling out [this form](https://docs.google.com/a/umich.edu/forms/d/e/1FAIpQLSfLnLd7cu_19JeL2kPCfCraAGYiEKTlXKi4nGJLKrcSk26pVQ/viewform)) but does not require an active Flux allocation.\n",
    "\n",
    "Note: When you create an account, you will automatically be added to our default queue. If you are using our Hadoop cluster for a class or you need a significant allocation, please <a href=\"mailto:hpc-support@umich.edu\">hpc-support@umich.edu</a> with us so you can be added to the appropriate queue. In many of the examples in this user guide, there is a “–queue <your_queue>” flag. Please fill in the name of your queue when running this examples, or simply “default” if you do not have one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing the cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the cluster, you will need a terminal.  Currently the cluster is only accessible via the commmand line. \n",
    "\n",
    "If you are trying to access the cluster from off campus, or via the MGuest wireless network, you will need one of two things:\n",
    "\n",
    "* [Install VPN software](http://www.itcom.itd.umich.edu/vpn/) on your computer\n",
    "* First ssh to login.itd.umich.edu, then ssh to cavium-thunderx.arc-ts.umich.edu from there.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Windows does not come with a suitable terminal/command line prompt, so extra software is needed to access the cluster.  PuTTY is probably the simplest to use.  [Click here](http://www.chiark.greenend.org.uk/~sgtatham/putty/) to download it.  \n",
    "\n",
    "Starting putty for the first time will bring up a window with an IP address, like so: ![putty login](https://arc-ts.umich.edu/wp-content/uploads/sites/4/2016/02/putty.png \"putty login\")\n",
    "\n",
    "Instead of flux-login.arc-ts.umich.edu, replace it with cavium-thunderx-login.arc-ts.umich.edu, and click on the open button and the bottom of the window.  When prompted, you will be asked for your uniqname and kerberos password.  After that you will be prompted for your second authentication token from Duo.  Enter that and you will be logged into the system and receive a command line prompt.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On Mac OS X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All OS X laptops and desktops come with a Terminal application under the Utilities folder \n",
    "\n",
    "![_](Utilities.jpeg \"bleh\")\n",
    "![_](Terminal.jpeg \"bleh\")\n",
    "\n",
    "Once you open a terminal window, you can simply type\n",
    "```\n",
    "ssh cavium-thunderx-login.arc-ts.umich.edu\n",
    "```\n",
    "And enter your kerberos password when prompted, followed by your Duo second factor token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On Linux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All linux workstations and servers come with a terminal application.  Start that terminal application, then type \n",
    "\n",
    "```\n",
    "ssh cavium-thunderx-login.arc-ts.umich.edu\n",
    "```\n",
    "And enter your kerberos password when prompted, followed by your Duo second factor token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Via a jupyter terminal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cavium Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is still some preparation to start using the Cavium cluster.  \n",
    "\n",
    "* Move data into HDFS to take full effect of the cluster.  \n",
    "* Transfer your code into the cluster and adapting it to use our versions of the software"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moving Data into HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hadoop consists of two components; HDFS, a filesystem built for high read speeds, and YARN, a resource manager. HDFS is not a POSIX filesystem, so normal command line tools like “cp” and “mv” will not work. Most of the common tools have been reimplemented for HDFS and can be run using the “hdfs dfs” command. All data must be in HDFS for jobs to be able to read it.\n",
    "\n",
    "We provide a staging volume for resaearchers to move data into the cluster.  We highly recommend using the umich#flux globus endpoint to quickly move data into and out of the cluster whenever possible when dealing with 1 TB or more of data.  The globus endpoint will allow access to the staging volume at 40 Gbps. \n",
    "\n",
    "If you do not have access to globus, researchers can also scp the data into the staging volume using the following command:\n",
    "\n",
    "```\n",
    "scp datafiles <your_username>@flux-xfer.arc-ts.umich.edu:/nfs/cavium-staging/<your_username>/\n",
    "```\n",
    "\n",
    "Once in the staging volume, you can use the HDFS native commands to put the data into your HDFS home directory as listed below.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HDFS native commands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are some basic commands: \n",
    "\n",
    "List the contents of your HDFS home directory\n",
    "```\n",
    "hdfs dfs -ls\n",
    "```\n",
    "Copy local file data.csv to your HDFS home directory\n",
    "```\n",
    "hdfs dfs -put data.csv data.csv\n",
    "```\n",
    "Copy HDFS file data.csv back to your local home directory\n",
    "```\n",
    "hdfs dfs -get data.csv data2.csv\n",
    "```\n",
    "A complete reference of HDFS commands can be found on the [Apache website](http://hadoop.apache.org/docs/r2.8.3/hadoop-project-dist/hadoop-common/FileSystemShell.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fuse HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fuse HDFS allows you use standard posix system commands with HDFS. This may be useful if you have a program that needs to use data that is stored in HDFS. It's also useful to move data into HDFS without relying on the staging volume.\n",
    "\n",
    "The downside is that we currently do not offer the globus endpoint to the Fuse HDFS, so the only mechanism to copy data to it is via scp:\n",
    "\n",
    "```\n",
    "scp data <your_username>@cavium-thunderx-login.arc-ts.umich.edu:/hadoop-fuse/user/<your_username>/\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Fuse HDFS commands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fuse HDFS can also be used to directly interact with your data via code like it's directly in the POSIX filesystem, rather than being limited to the HDFS commands.\n",
    "\n",
    "To use Fuse HDFS, change directories to /hadoop-fuse/user/<your_uniqname>\n",
    "\n",
    "```\n",
    "cd /hadoop-fuse/user/<your_username>\n",
    "```\n",
    "\n",
    "Once in this directory, you can use commands on your HDFS files just as you would on any other files. For example, the ls command will list the contents of your HDFS home directory.\n",
    "\n",
    "You could also run a Python or R program that uses a file in HDFS.\n",
    "\n",
    "You can save the below file and run it as you would regularly run a python program to access an example data file we have available to all users in HDFS.\n",
    "\n",
    "```\n",
    "#!/usr/bin/python\n",
    "f = open(\"/hadoop-fuse/var/examples/romeojuliet.txt\", \"r\")\n",
    "data = f.read()\n",
    "f.close()\n",
    "d = {}\n",
    "for word in data.split(' '):\n",
    "        if word in d:\n",
    "                d[word] += 1\n",
    "        else:\n",
    "                d[word] = 1\n",
    "for word, count in d.items():\n",
    "        print word + str(count)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Spark on Cavium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark and PySpark utilize a container called Resilient Distributed Dataset (RDD) for storing and operating on data. The most important characteristic of Spark’s RDD is that it is immutable — once created, the data it contains cannot be updated. New RDDs can be created by transforming the data in another RDD, which is how analysis is done with Spark.\n",
    "\n",
    "Using Spark’s native language, Scala, requires more setup than using PySpark. Some example Scala jobs, including the same example job in the PySpark documentation, can be found on this website. That Spark code has some trivial set up required to run a Spark job, and all of the actual logic is in the ‘run’ function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Pyspark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Spark comes with an interactive Python console, which can be opened this way:\n",
    "```\n",
    "# Load the pyspark console \n",
    "pyspark --master yarn --queue <your_queue>\n",
    "```\n",
    "This interactive console can be used for prototyping or debugging, or just running simple jobs.\n",
    "\n",
    "The following example runs a simple line count on a text file, as well as counts the number of instances of the word “words” in that textfile. You can use any text file you have for this example:\n",
    "```\n",
    ">>> textFile = sc.textFile(\"test.txt\")\n",
    ">>> textFile.count()\n",
    ">>> textFile.first()\n",
    ">>> textFile.filter(lambda line: \"words\" in line).count()\n",
    "```\n",
    "\n",
    "You can also submit a job using PySpark without using the interactive console.\n",
    "\n",
    "Save this file as job.py.\n",
    "```\n",
    "from pyspark import SparkConf, SparkContext\n",
    "import sys\n",
    "\n",
    "# This script takes two arguments, an input and output\n",
    "if len(sys.argv) != 3:\n",
    "  print('Usage: ' + sys.argv[0] + ' <in> <out>')\n",
    "  sys.exit(1)\n",
    "\n",
    "input = sys.argv[1]\n",
    "output = sys.argv[2]\n",
    "\n",
    "# Set up the configuration and job context\n",
    "conf = SparkConf().setAppName('AnnualWordLength')\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "\n",
    "# Read in the dataset and immediately transform all the lines in arrays\n",
    "data = sc.textFile(input).map(lambda line: line.split('\\t'))\n",
    "\n",
    "# Create the 'length' dataset as mentioned above. This is done using the next two variables, and the 'length' dataset ends up in 'yearlyLength'.\n",
    "yearlyLengthAll = data.map(\n",
    "    lambda arr: (int(arr[1]), float(len(arr[0])) * float(arr[2]))\n",
    ")\n",
    "yearlyLength = yearlyLengthAll.reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# Create the 'words' dataset as mentioned above.\n",
    "yearlyCount = data.map(\n",
    "    lambda arr: (int(arr[1]), float(arr[2]))\n",
    ").reduceByKey(\n",
    "    lambda a, b: a + b\n",
    ")\n",
    "\n",
    "# Create the 'average_length' dataset as mentioned above.\n",
    "yearlyAvg = yearlyLength.join(yearlyCount).map(\n",
    "    lambda tup: (tup[0], tup[1][0] / tup[1][1])\n",
    ")\n",
    "\n",
    "# Save the results in the specified output directory.\n",
    "yearlyAvg.saveAsTextFile(output)\n",
    "\n",
    "# Finally, let Spark know that the job is done.\n",
    "sc.stop()\n",
    "```\n",
    "This above script averages the lengths of words in the NGrams dataset by year. There are two main operations in the above code: ‘map’ and ‘reduceByKey’. ‘map’ applies a function to each RDD element and returns a new RDD containing the results. ‘reduceByKey’ applies a function to the group of values with the same key – for all keys – and returns an RDD with the result.\n",
    "\n",
    "The job can be submitted by running:\n",
    "```\n",
    "spark-submit \\\n",
    " --master yarn \\\n",
    " --num-executors 35 \\\n",
    " --executor-memory 5g \\\n",
    " --executor-cores 4 \\\n",
    " job.py /var/ngrams/data ngrams-out\n",
    "\n",
    "\n",
    "hdfs dfs -cat ngrams-out/*\n",
    "```\n",
    " \n",
    "\n",
    "The only required argument from the above job submission command is ‘–master yarn’. The values passed to the other arguments may be modified in order to get better performance or conform to the limits of your queue.\n",
    "\n",
    "Note: Our default Python is python x.x.x. We currently do not use the Conda distribution as there is not a version of Conda for the 64 bit ARM archtitecture at this time.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Spark Shell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark has an easy-to-use interactive shell that can be used to learn API and also analyze data interactively. Below is a simple example written in Scala. You can use any text file that you have:\n",
    "```\n",
    "spark-shell --master yarn --queue <your_queue>\n",
    "scala> val textFile = spark.read.textFile(\"test.txt\")\n",
    "scala> textFile.count()\n",
    "scala> textFile.first()\n",
    "//Count how many lines contain the word \"words\"\n",
    "//You can replace \"words\" with any word you'd like\n",
    "scala> textFile.filter(line => line.contains(\"words\")).count()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Spark Submit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is a simple example of submitting a Spark job that uses an existing jar all users have access to. It estimates Pi, and the number at the end is the number of iterations it uses (more iterations = more accurate).\n",
    "```\n",
    "export SPARK_MAJOR_VERSION=2\n",
    "cd /usr/hdp/current/spark2-client\n",
    "spark-submit \\\n",
    "   --class org.apache.spark.examples.SparkPi \\\n",
    "   --master yarn \\\n",
    "   --queue <your_queue> \\\n",
    "examples/jars/spark-examples*.jar 10\n",
    "```\n",
    "Gradle is a popular build tool for Java and Scala. The following example is useful if you may be getting code from bitbucket, github, etc. This code can be downloaded and built by logging on to cavium-thunderx-arc-ts.umich.edu and running:\n",
    "```\n",
    "git clone https://bitbucket.org/umarcts/spark-examples\n",
    "cd spark-examples\n",
    "./gradlew jar\n",
    "```\n",
    "The last command, “./gradlew jar”, will download all dependencies, compile the code, run tests, and package all of the code into a Java ARchive (JAR). This JAR is submitted to the cluster to run a job. For example, the AverageNGramLength job can be launched by running:\n",
    "```\n",
    "spark-submit \\\n",
    "   --class com.alectenharmsel.examples.spark.AverageNGramLength \\\n",
    "   --master yarn \\\n",
    "   --executor-memory 3g \\\n",
    "   --num-executors 35 \\\n",
    " build/libs/spark-examples-*-all.jar /var/ngrams/data ngrams-out\n",
    "```\n",
    "The output will be located in your home directory in a directory called ‘ngrams-out’, and can be viewed by running:\n",
    "```\n",
    "hdfs dfs -cat ngrams-out/* | tail -5\n",
    "```\n",
    "\n",
    "The output should look like this:\n",
    "\n",
    "![spark output](https://arc-ts.umich.edu/wp-content/uploads/sites/4/2016/03/spark-output.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using SparkR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "SparkR allows users to utilize the ease of data analysis in R while using the speed and capacity of Spark on our Hadoop cluster. Those familiar with R should have no problem utilizing this feature. After opening the SparkR session, simply begin typing out your program in R.\n",
    "\n",
    "Run this to open a SparkR session, run this:\n",
    "```\n",
    "sparkR --master yarn --queue <your_queue> --num-executors 4 --executor-memory 1g --executor-cores 4\n",
    "``` \n",
    "\n",
    "The following is an example you can run to get a feel for how SparkR works. This example was taken from the official SparkR documentation, which can be found here, along with other examples.\n",
    "```\n",
    "families <- c(\"gaussian\", \"poisson\")\n",
    "train <- function(family) {\n",
    " model <- glm(Sepal.Length ~ Sepal.Width + Species, iris, family = family)\n",
    " summary(model)\n",
    "}\n",
    "# Return a list of model's summaries\n",
    "model.summaries <- spark.lapply(families, train)\n",
    "\n",
    "# Print the summary of each model\n",
    "print(model.summaries)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you’re familiar with Spark, you know that a dataframe is essentially a data structure that contains “tabular” data in memory. That is, it consists of rows and columns of data that can, for example, store the results of an SQL-style query. Dataframes can be saved into HDFS as Parquet files. Parquet files not only preserve the schema information of the dataframe, but will also compress the data when it gets written into HDFS. This means that the saved file will take up less space in HDFS and it will load faster if you read the data again later. Therefore, it is a useful storage format for data you may want to analyze multiple times.\n",
    "\n",
    "The Pyspark example below uses Reddit data which is available to all Cavium Hadoop users in HDFS ‘/var/reddit’. This data consists of information about all posts made on the popular website Reddit, including their score, subreddit, text body, author, all of which can make for interesting data analysis.\n",
    "\n",
    "#First, launch the pyspark shell\n",
    "\n",
    "```\n",
    "pyspark --master yarn --queue <your_queue> --num-executors 35 --executor-cores 4 --executor-memory 5g\n",
    "\n",
    "#Load the reddit data into a dataframe\n",
    "\n",
    ">>> reddit = sqlContext.read.json(\"/var/reddit/RS_2016-0*\")\n",
    "\n",
    "#Set compression type to snappy\n",
    "\n",
    ">>> sqlContext.setConf(\"spark.sql.parquet.compression.codec\", \"snappy\")\n",
    "\n",
    "#Write data into a parquet file - this example puts it into your HDFS home directory as “reddit.parquet”\n",
    "\n",
    ">>> reddit.write.parquet(\"reddit.parquet\")\n",
    "\n",
    "#Create a new dataframe from parquet file \n",
    "\n",
    ">>> parquetFile = sqlContext.read.parquet(\"reddit.parquet\")\n",
    "\n",
    "#Register dataframe as a SQL temporary table\n",
    "\n",
    ">>> parquetFile.registerTempTable(“reddit_table\")\n",
    "\n",
    "#Query the table\n",
    "\n",
    "#Can really be any query, but this query will find some of the more highly rated posts\n",
    "\n",
    ">>> ask = sqlContext.sql(“SELECT title FROM reddit_table WHERE score > 1000 and subreddit = ‘AskReddit’”)\n",
    "\n",
    "#Since we created the dataframe “ask” with the previous query, we can write it out to HDFS as a parquet file so it can be accessed again later\n",
    "\n",
    ">>> ask.write.parquet(“ask.parquet”)\n",
    "\n",
    "#Exit the pyspark console - you’ll view the contents of your parquet file after\n",
    "\n",
    ">>> exit()\n",
    "``` \n",
    "\n",
    "To view the contents of your Parquet file, use Parquet tools. Parquet tools is a command line tool that aids in the inspection of Parquet files, such as viewing its contents or its schema.\n",
    "```\n",
    "#view the output \n",
    "\n",
    "hadoop jar /sw/dsi/noarch/parquet-tools-1.7.0.jar cat \\ \n",
    "ask.parquet\n",
    "\n",
    "#view the schema; in this case, just the “title” of the askreddit thread \n",
    "\n",
    "hadoop jar /sw/dsi/noarch/parquet-tools-1.7.0.jar schema \\ \n",
    "ask.parquet \n",
    "\n",
    "#to get a full list of all of the options when using Parquet tools  \n",
    "hadoop jar /sw/dsi/noarch/parquet-tools-1.7.0.jar -h\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section will be for other tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section is for policies for using the Cavium Data Science Cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requesting new tools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you'd like to request new tools, feel free to email <a href=\"mailto:hpc-support@umich.edu\">hpc-support@umich.edu</a>.  Because of the pilot nature of the Cavium Data Science environment, it may not be possible to install it immediately.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Management\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of the shared nature of the staging volume in the Cavium Data Science cluster, data has to have an active data lifecycle before it must be erased.  We will hold to a schedule of 4 weeks for data on the staging volume before it must be deleted.  The staging volume currently has 50 TB of space, so we must be mindful of usable space avsilable to all.  \n",
    "\n",
    "Similarly, data within HDFS will also have a lifecycle, but exact details are yet to be determined.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keeping another copy of your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recommend that all researchers back up a copy of your data, especially for very large datasets. You may wish to do so locally, but also we do offer the Data Den service for people who need to keep very large datasets cheaply.  It is close to the Cavium cluster for access, it will take some time to copy data back to the Data Science cluster, but it will be faster to retrieve.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [anaconda]",
   "language": "python",
   "name": "Python [anaconda]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
